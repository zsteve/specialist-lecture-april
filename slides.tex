
\documentclass{beamer}
\usetheme{default}
\title{ATAR Notes Specialist Maths Term 1 Lecture}
\author{Stephen Zhang}

\usepackage{pgfpages}
\renewcommand{\d}{\hspace{0.2cm}\text{d}}

\setbeamertemplate
{footline}{\quad\hfill\insertframenumber\strut\quad\quad\quad} 
\begin{document}

\mode<handout>{\setbeamercolor{background canvas}{bg=black!20}}
\pgfpagesuselayout{2 on 1}[a4paper]


\begin{frame}
	\titlepage
\end{frame}

\begin{frame}{Plan for today}
	\begin{itemize}
		\item \textbf{Session 1} \textbf{Vectors} and \textbf{Complex Numbers Part 1}
		\item \textbf{Session 2} \textbf{Complex Numbers Part II} and \textbf{Calculus intro}
		\item \textbf{Session 3} \textbf{More calculus} ... than you'll ever want to know
	\end{itemize}
\end{frame}

\begin{frame}{What's up?}
	\begin{itemize}
		\item Hope year 12 is going well for you guys =) it's been a while
		\item Take a chill pill and relax cos it's gonna be fun guys
		\item Having trouble with Specialist at this point of year? Make use of your holidays!
		\item Don't underestimate the power of \textbf{notetaking}
	\end{itemize}
\end{frame}

\begin{frame}{What's up?}
	\textbf{Notetaking}\\~\\
	Writing up \textbf{notes} for maths (and also other courses) is super important.
	\begin{itemize}
		\item Forces you to \textbf{work through} problems and derivations
		\item Don't write down a formula without understanding
	\end{itemize}
	\begin{center}
		\textbf{No point reading or listening about math ... need to work through with paper and pencil}
	\end{center}
\end{frame}

\begin{frame}{I present you this...}
	\includegraphics[width = \linewidth]{img/quote1.png}\\
	From \textit{S. Thompson, Calculus Made Easy (1914)}
 \end{frame}

\begin{frame}{So strap yourselves in...}
	\begin{center}
		\includegraphics[scale=0.80]{img/ludicrous_speed.jpg}
	\end{center}
	\end{frame}

\section{Unit 3 recap}

\subsection{Kindergarten}
\begin{frame}
	You should recognise these formulas and know how to use them...
	\textbf{Geometric progression and series}
	\begin{itemize}
		\item $\sum_{k = 0}^{n} ar^k = a + ar + ... + ar^n = \dfrac{a(1-r^n)}{1-r}$
		\item $\sum_{k = 0}^{\infty} ar^k = a + ar + ar^2 + ... = \dfrac{a}{1-r}$\\
		$\Rightarrow$ ... under what conditions?
	\end{itemize}
	\textbf{Ellipses and hyperbolae}
	\begin{itemize}
		\item $\dfrac{x^2}{a^2} + \dfrac{y^2}{b^2} = 1 \Rightarrow \text{ellipse}$
		\item $\dfrac{x^2}{a^2} - \dfrac{y^2}{b^2} = 1 \Rightarrow \text{hyperbola... what kind?}$
		\item $\dfrac{y^2}{b^2} - \dfrac{x^2}{a^2} = 1 \Rightarrow \text{hyperbola... what kind?}$
	\end{itemize}
	Of course we can have the necessary translations: $x \to x-h$ ; $y \to y-k$.
\end{frame}

\begin{frame}
	\textbf{Example 1:} Plot the ellipse with equation:
	$\dfrac{x^2}{3} + \dfrac{y^2}{4} = 1$
	Hence, draw the hyperbola with equation:
	$\dfrac{x^2}{3} - \dfrac{y^2}{4} = 1$
	\vspace{5cm}
\end{frame}

\begin{frame}{Geometry}
	\begin{itemize}
		\item Circle theorems (refer to Year 11)
		\item Sine rule:
			$$\dfrac{\sin(A)}{a} = \dfrac{\sin(B)}{b} = \dfrac{\sin(C)}{c}$$
		\item Cosine rule:
			$$c^2 = a^2 + b^2 - 2ab\cos(C)$$
		\item Pythagoras theorem (and its converse)
			$$a^2 + b^2 = c^2 \Leftrightarrow \text{ABC is a right angled triangle}$$
	\end{itemize}
\end{frame}

\subsection{Vectors}
\begin{frame}{Vectors}
\begin{itemize}
	\item \textbf{Vectors} are a general class of mathematical objects that satisfy some expected properties. \textbf{For our purposes}, vectors can be treated as just 'arrows'
	\item \textbf{Vectors} are special because they satisfy a very specific set of properties - they come from \textbf{vector spaces}, e.g. $\mathbb{R}^2, \mathbb{R}^3$
	\item \textbf{Geometric interpretation:} vectors are directed line segments with a \textbf{head} and \textbf{tail}
\end{itemize}
\end{frame}

\begin{frame}{Vector toolbox}
	\begin{itemize}
		\item \textbf{Scalar multiplication: }\\$\vec{v} \to \lambda \vec{v}$ stretches $\vec{v}$ by $|\lambda|$ and possibly reverses direction.
		\item \textbf{Addition: }\\$\vec{a} + \vec{b} = \vec{b} + \vec{a}$, add head $\to$ tail
		\item \textbf{Subtraction: }\\ $\vec{a} - \vec{b} = \vec{a} + (-1)\vec{b}$
		\item \textbf{Magnitude: }\\$|\vec{a}| = \text{length of }\vec{a}$, $|\vec{a}| \ge 0$
		\item \textbf{Scalar product: }\\ $\vec{a}\cdot\vec{b} = |\vec{a}||\vec{b}|\cos(\theta)$
		\item \textbf{Unit vector: }\\ $\hat{a} = \dfrac{\vec{a}}{|\vec{a}|}$
	\end{itemize}
\end{frame}

\begin{frame}{Vector toolbox}
	\begin{center}
	\includegraphics[width = 0.75\linewidth]{img/vector_addition.png}
	\end{center}
\end{frame}

\begin{frame}{Vector toolbox}
	\textbf{Scalar product}\\~\\
	The scalar ($\equiv$ dot) product between two vectors is super useful.
	
	$$\vec{u} \cdot \vec{v} = |\vec{u}||\vec{v}|\cos(\theta)$$
	
	In fact, the \textbf{scalar product} is what defines the concept of \textbf{distance} and \textbf{orientation} in our vector space.
	\\~\\
	For instance, the magnitude of a vector is formally defined as:
	
	$$|\vec{u}| = \sqrt{\vec{u}\cdot\vec{u}}$$
	
	The scalar product we are using is the \textbf{Euclidean} inner product. There are other strange beasts out there, though, that give us \textit{distorted} concepts of length.
\end{frame}

\begin{frame}{Vector toolbox}
	\textbf{Vector projections}
	Given two vectors $\vec{a}, \vec{b}$, we can consider $\vec{a}_\parallel$, the projection of $\vec{a}$ onto $\vec{b}$
	$$\vec{a}_\parallel = \left(\dfrac{\vec{a}\cdot\vec{b}}{\vec{b}\cdot\vec{b}}\right)\vec{b} = (\vec{a}\cdot\hat{b})\hat{b}$$
	\begin{center}
	\includegraphics[width=0.50 	\linewidth]{img/vector_projection.png}
	\end{center}
\end{frame}

\begin{frame}{Vector toolbox}
	\textbf{Vector projection}
	Once we have $\vec{a}_\parallel$, $\vec{a}_\perp$ can simply be found by subtracting:
	
	$$\vec{a}_\perp = \vec{a} - \vec{a}_\parallel$$
	
	\begin{center}
	\includegraphics[width=0.50 	\linewidth]{img/vector_projection.png}
	\end{center}
\end{frame}

\begin{frame}{Ratio theorem}
	This is an important theorem...
	$$\vec{r} = \dfrac{\mu\vec{a} + \lambda \vec{b}}{\lambda + \mu}$$
	\begin{center}
	\includegraphics[scale = 0.40]{img/ratio_theorem.png}
	\end{center}
\end{frame}

\begin{frame}{Cartesian representation}
	We've been speaking about vectors in an abstract manner. In $\mathbb{R}^2, \mathbb{R}^3$, we can \textbf{project} a vector $\vec{a}$ onto a \textbf{basis} $\{\vec{i}, \vec{j}, \vec{k}\}$
	\begin{align*}
		\vec{a} &= (\vec{a}\cdot\vec{i})\vec{i} + (\vec{a}\cdot\vec{j})\vec{j} + (\vec{a}\cdot\vec{k})\vec{k}\\
				&= a_1\vec{i} + a_2\vec{j} + a_3\vec{k}\\
				&= (a_1, a_2, a_3)\\
	\end{align*}
	
	We say that $a_1, a_2, a_3$ are \textbf{projections} of our vector $\vec{a}$ in the $x, y, z$ directions respectively. Can we derive this from our previous definitions of \textbf{vector projections}?
\end{frame}

\begin{frame}{Cartesian representation}
	Using the \textbf{Cartesian representation} of vectors, we find that:
	\begin{itemize}
		\item \textbf{Scalar multiplication: } $\lambda(a_1, a_2, a_3) = (\lambda a_1, \lambda a_2, \lambda a_3)$
		\item \textbf{Addition: } $(a_1, a_2, a_3) + (b_1, b_2, b_3) = (a_1 + b_1, a_2 + b_2, a_3 + b_3)$
		\item \textbf{Magnitude: } $|(a_1, a_2, a_3)| = \sqrt{a_1^2 + a_2^2 + a_3^2}$
		\item \textbf{Scalar product: } $(a_1, a_2, a_3)\cdot(b_1, b_2, b_3) = a_1b_1 + a_2b_2 + a_3b_3$
	\end{itemize}
\end{frame}

\begin{frame}{Pythagoras in 3D}
	We said previously that
	$$|\vec{a}| = \sqrt{a_1^2 + a_2^2 + a_3^2}$$
	
	This is \textbf{Pythagoras' theorem} for 3 dimensions. Proof?
	\begin{center}
		\includegraphics[scale = 0.5]{img/axes.png}
	\end{center}
\end{frame}

\begin{frame}
	\includegraphics[width = \linewidth]{img/vector_meme.jpg}
\end{frame}

\begin{frame}{Linear independence}
	\textbf{Any} set of vectors will either be linearly \textbf{dependent} or otherwise \textbf{independent}.\\~\\
	
	\textbf{Formal definition}\\~\\
	
	$\vec{a}, \vec{b}, \vec{c}$ are \textbf{linearly dependent} if there exists $k_1, k_2, k_3$ \textbf{not all zero} such that:
	
	$$k_1\vec{a} + k_2\vec{b} + k_3\vec{c} = \vec{0}$$
	
	\textbf{Practical definition}\\~\\
	
	$\vec{a}, \vec{b}, \vec{c}$ are \textbf{linearly dependent} if we can write \textbf{one vector} as a \textbf{linear combination} of the others.
	
	E.g. $\vec{c} = m\vec{a} + n\vec{b}$, and so on.
\end{frame}

\begin{frame}{Linear independence}
	\begin{align*}
		\vec{a} &= \vec{i} + 3\vec{j} + \vec{k}\\
		\vec{b} &= -2\vec{j} + 5\vec{k}\\
		\vec{c} &= 2\vec{i} + 17\vec{k}
	\end{align*}
	
	$\{\vec{a}, \vec{b}, \vec{c}\}$ are \textbf{linearly dependent} since
	$\vec{c} = 2\vec{a} + 3\vec{b}$
\end{frame}

\begin{frame}{Linear independence}
	If a set of vectors is \textbf{not} linearly dependent, then it is \textbf{linearly independent}.
	
	\textbf{Useful special cases:}
	\begin{itemize}
		\item 2 vectors are dependent only if they are \textbf{parallel}
		\item In 2 dimensions, any set of $>$2 vectors will be dependent
		\item In 3 dimensions, any set of $>$3 vectors will be dependent
		\item Any 3 \textbf{coplanar} vectors will be dependent
	\end{itemize}
\end{frame}

\begin{frame}
	\includegraphics[width = \linewidth]{img/indep_1.png}
\end{frame}

\begin{frame}
	\includegraphics[width = \linewidth]{img/indep_2.png}
\end{frame}

\begin{frame}
	Example: VCAA 2009 Exam 2
	\includegraphics[width = \linewidth]{img/q1.png}
\end{frame}

\begin{frame}
	\textbf{A trick for nerds}\\~\\
	Consider the vectors $\vec{a} = (a_1, a_2, a_3),\quad\vec{b} = (b_1, b_2, b_3),\quad\vec{c} = (c_1, c_2, c_3)$.
	\\~\\
	\begin{center}$\vec{a}, \vec{b}, \vec{c}$ are \textbf{linearly independent} \textit{if and only if}\end{center}
	
	$$\begin{vmatrix}a_1&a_2&a_3\\b_1&b_2&b_3\\c_1&c_2&c_3
	\end{vmatrix} = 0$$
	
	This determinant is easy to check using a CAS.
\end{frame}

\begin{frame}
	Example: VCAA 2009 Exam 1
	\includegraphics[width = \linewidth]{img/q2.png}
\end{frame}

\begin{frame}

	\textbf{Challenge problem}
	\\~\\
	Using vector projections, find the point on the line $y = 3x$ that is closest to the point $(4, 2)$. 
	What is the minimal distance between the point $(4, 2)$ and the line?	
	
	\vspace{5cm}

\end{frame}

\begin{frame}{Vector proofs}
	\textbf{Vector proofs} involve a wide range of techniques, but in general...\\~\\
	\begin{itemize}
		\item We represent vectors using a \textbf{general notation} e.g. $\vec{a}$, \textbf{not} not in component ($\vec{i},\vec{j},\vec{k}$) form.
		\item \textbf{Dot product} is one of the most valuable tools you have!

		$$\vec{a}\cdot\vec{b} = |\vec{a}||\vec{b}|\cos(\theta)$$
		
		$$\vec{a}\cdot\vec{a} = |\vec{a}|^2$$		
		
		\item Start with what you want to prove - \textbf{draw a diagram}
	\end{itemize}
\end{frame}

\begin{frame}
	\textbf{Problem: } Prove the cosine rule.
	\vspace{5cm}
\end{frame}

\section{Complex numbers}

\begin{frame}{Complex numbers basics}
	\textbf{Definition of a complex number...}
	
	$$\mathbb{C} = \{z = x + iy : x, y \in \mathbb{R} \}$$
	
	
	\textbf{Complex numbers} follow the basic arithmetic and algebraic properties we are used to applying to \textbf{reals}.
	\\~\\
	\textbf{However}, we do have some extras...

\end{frame}

\begin{frame}{Complex conjugate}
	The \textbf{conjugate} of a complex number $z$ is denoted $\bar{z}$, and:
	
	$$z = x + iy \Rightarrow \bar{z} = x - iy$$
	
	The conjugate satisfies some \textbf{very important} properties...
	\begin{itemize}
		\item $z\bar{z} = (x + iy)(x - iy) = |z|^2$
		\item $z + \bar{z} = 2\mathrm{Re}(z)$
		\item $z - \bar{z} = 2i\mathrm{Im}(z)$
		\\~\\
		\item $\overline{z + w} = \bar{z} + \bar{w}$
		\item $\overline{zw} = \bar{z}\bar{w}$
		\item $\overline{z/w} = \bar{z}/\bar{w}$
	\end{itemize}
	
	
\end{frame}

\begin{frame}{Polar form}
	Complex numbers can be expressed in \textbf{polar form} - this makes multiplication, division and exponentiation a lot easier.
	
	$$z = r\mathrm{cis}(\theta) = r(\cos(\theta) + i \sin(\theta))$$
	\vspace{5cm}
	
	We write $r = |z|$ and $\theta = \mathrm{arg}(z)$
\end{frame}

\begin{frame}{Polar form}
	\textbf{Polar form} makes some things much easier...
	\begin{itemize}
		\item $z_1 z_2 = r_1 r_2 \mathrm{cis}(\theta_1 + \theta_2)$
		\item $z_1/z_2 = \dfrac{r_1}{r_2}\mathrm{cis}(\theta_1 - \theta_2)$
		\item $z^n = r^n \mathrm{cis}(n\theta) \quad \leftarrow \quad \text{De Moivre's Theorem}$
	\end{itemize}
	
	Importantly for the \textbf{conjugate}, we have:
	
	$$z = r\mathrm{cis}(\theta) \Rightarrow \overline{z} = r\mathrm{cis}(-\theta)$$
\end{frame}

\begin{frame}
	\textbf{Problem.} Find in \textbf{Cartesian form}: $u = \mathrm{cis}(\pi/4), v = \mathrm{cis}(\pi/3)$.\\~\\
	Hence, find the values of $\sin(\pi/12)$ and $\cos(\pi/12)$
	\vspace{5cm}
\end{frame}

\begin{frame}{Factorisation over $\mathbb{C}$}
	\textbf{Fundamental Theorem of Algebra (formal statement)}\\
	Every polynomial in $P$ in $z$:
	$$P(z) = a_n z^n + a_{n-1}z^{n-1} + ... + a_2 z^2 + a_1 z + a_0$$
	where $a_n \ne 0$ and $a_0, ..., a_n \in \mathbb{C}$ has at least \textbf{one} linear factor in $\mathbb{C}$.
	\\~\\
	\textbf{Version for p l e b s:}\\
	Any polynomial of degree $n$ will have $n$ linear factors over $\mathbb{C}$.
\end{frame}

\begin{frame}
	\textbf{Quadratics: } we typically \textbf{complete the square}... best shown by example.
	\\~\\
	Factorise $z^2 + 2z + 5$
	\vspace{5cm}
	\\
	For higher order polynomials typically you'll need to use the \textbf{remainder theorem} to reduce to a quadratic
\end{frame}

\begin{frame}
	\textbf{Conjugate factor theorem}\\~\\
	If the polynomial $P(z)$ has \textbf{real coefficients} and $P(\alpha) = 0$ where $\alpha \in \mathbb{C}$, then $P(\overline{\alpha}) = 0$ also.\\~\\
	I.e. if $(z - \alpha)$ is a factor, then so is $(z - \overline{\alpha})$
\end{frame}

\begin{frame}
	\textbf{Problem: } VCAA 2014 Exam 1
	\includegraphics[width=\linewidth]{img/q3a.png}
	\vspace{4cm}
\end{frame}

\begin{frame}
	\textbf{Problem: } VCAA 2014 Exam 1
	\includegraphics[width=\linewidth]{img/q3b.png}
	\vspace{4cm}
\end{frame}

\begin{frame}{plz don't kill me for this}
	\begin{center} who would win??!!!1111 \end{center}

	\begin{align*}
		&\text{An irreducible quadratic}\quad\quad\quad\quad&\text{one dreamy boi}\\
		& z^2 + 2x + 2 & z = x + iy
	\end{align*}	
\end{frame}

\begin{frame}{$n$th roots}
	We can find the $n$th roots of any complex number using de Moivre's Theorem in the following procedure...
	
	$$z^n = a, n \in \mathbb{N}$$
	
	\begin{itemize}
		\item There will be $n$ solutions $w_1, ..., w_n$ (c.f. Fundamental Theorem of Algebra)
		\item Solutions will lie on the circle with radius $|a|^{1/n}$ at even intervals of $2\pi/n$.
	\end{itemize}
\end{frame}

\begin{frame}
	\textbf{Example:} find the sixth roots of unity.
	\vspace{5cm}
\end{frame}

\begin{frame}
	\begin{center}
	\includegraphics[width=0.5\linewidth]{img/quote2.png}
	\end{center}
	From \textit{S. Thompson, Calculus Made Easy (1914)}
\end{frame}

\begin{frame}{Differentiation}
	\textbf{Assumed knowledge: } most of Methods calculus. For those of you who haven't seen...
	\begin{align*}
		\dfrac{d}{dx} \sin(x) &= \cos(x)\\
		\dfrac{d}{dx} \cos(x) &= -\sin(x)\\
		\dfrac{d}{dx} \tan(x) &= \sec^2(x)\\
		\dfrac{d}{dx} \ln(x) &= \dfrac{1}{x}\\
		\dfrac{d}{dx} e^x &= e^x
	\end{align*}
	
	Also the \textbf{chain rule}:
	
	$$\dfrac{dy}{dx} = \dfrac{dy}{dt} \dfrac{dt}{dx}$$
	
\end{frame}

\begin{frame}{A few new functions...}
	You will have encountered already...
	\begin{align*}
		\arcsin(x) &= \sin^{-1}(x)\\
		\arccos(x) &= \cos^{-1}(x)\\
		\arctan(x) &= \tan^{-1}(x)
	\end{align*}

We seek their \textbf{derivatives...}

\begin{align*}
	\arcsin'(x) &= \dfrac{1}{\sqrt{1-x^2}}\\
	\arccos'(x) &= \dfrac{-1}{\sqrt{1-x^2}}\\
	\arctan'(x) &= \dfrac{1}{1+x^2}
\end{align*}
\end{frame}

\begin{frame}
	Let's derive these ...
	
	$$\arctan'(x) = \dfrac{1}{1+x^2}$$
	
	Let $y = \tan(x), \quad x \in (-\pi/2, \pi/2), \quad y \in \mathbb{R}$. Thus $x = \arctan(y)$.
	\vspace{5cm}
\end{frame}

\begin{frame}{The modulus function}
	You've probably seen already the modulus function...
	$$|x| = x\mathrm{sgn(x)} = \text{'positive value of x'}$$
	
	From the graph of $|x|$ we see that our function is \textbf{continuous everywhere}, and \textbf{differentiable} away from the origin.
	
	\begin{center}\includegraphics[scale=0.10]{img/abs.png}\end{center}
	
	$$\dfrac{d}{dx} |x| = \begin{cases}+1, x > 0\\ -1, x < 0\end{cases}$$
\end{frame}

\begin{frame}{Implicit differentiation}
	So far, we have been able to differentiate expressions where $y$ is \textbf{explicitly} related to $x$.
	\\~\\	
	Implicit relations allow us to find derivatives where $y$ and $x$ are \textbf{implicitly} tangled together... 
	\begin{center}for example in the expression $x \cos(y) + y\cos(x) = 1$\end{center}
	
	\textbf{Quick rule: } to find $dy/dx$, differentiate as per usual w.r.t $x$, using chain/product/quotient rules as required, but write 
	$$\dfrac{d}{dx}(y) = \dfrac{dy}{dx} \quad \text{(obviously!!!)}$$
\end{frame}

\begin{frame}
	\textbf{Example:} given that $x \cos(y) + y \cos(x) = 1$, find an expression for $dy/dx$.
	\vspace{5cm}
\end{frame}

\begin{frame}
	Here's a cheap way to do the same thing...\\~\\
	To differentiate some relation, rearrange into the form $F(x, y) = 0$ and then find \textbf{partial derivatives} $F_x, F_y$. From this, we have:
	
	$$\dfrac{dy}{dx} = -\dfrac{F_x}{F_y}$$

	\begin{itemize}
		\item To find $F_x$, differentiate $F(x, y)$ w.r.t $x$, treating $y$ like a \textbf{constant}
		\item To find $F_y$, differentiate $F(x, y)$ w.r.t $y$, treating $x$ like a \textbf{constant}
	\end{itemize}
\end{frame}

\begin{frame}{Concavity and the second derivative test}
	\begin{itemize}
		\item We know that \textbf{stationary points} can be a local \textbf{maximum}, \textbf{minimum}, or \textbf{point of inflection}
		\item \textbf{Without} sketching a graph, how would we be able to classify stationary points of a graph?
		\item We first introduce the idea of \textbf{concavity}
	\end{itemize}
\end{frame}

\begin{frame}{Concavity}
	\textbf{Concavity} of a curve at a given point tells us whether the curve is 'curving upwards' or 'curving downwards'
	\begin{itemize}
		\item Consider the second derivative $f''(x) = \dfrac{d^2f}{dx^2}$
		\item \textbf{Case: } $f''(x) > 0$ $\Rightarrow$ concave up (slope is increasing)
		\item \textbf{Case: } $f''(x) < 0$ $\Rightarrow$ concave down (slope is decreasing)
		\item \textbf{Case: } $f''(x) = 0$ $\Rightarrow$ zero concavity, i.e. locally our curve has no 'curvature'
	\end{itemize}
\end{frame}

\begin{frame}{Second derivative test}
	Thus, given a \textbf{stationary point} at $x = x_0$, we can check the value of $f''(x_0)$ to determine the type of stationary point.
		
	\begin{itemize}
		\item $f''(x_0) > 0$ $\Rightarrow$ concave up $\Rightarrow$ local minimum
		\item $f''(x_0) < 0$ $\Rightarrow$ concave down $\Rightarrow$ local maximum
		\item $f''(x_0) = 0$ $\Rightarrow$ no concavity $\Rightarrow$ inconclusive
	\end{itemize}
	
	This is the \textbf{second derivative test.}
\end{frame}


\begin{frame}{Second derivative test}
	\textbf{What if $f''(x_0) = 0$?}
	\\~\\
	If $f''$ \textbf{switches sign} around $x_0$...
	\begin{itemize}
		\item We know that $y = f(x)$ changes its concavity around $x_0$
		\item Thus $y = f(x)$ has a \textbf{point of inflection} at $x_0$
		\item Depending on whether $f'(x_0) = 0$\\	this could be a \textbf{stationary point of inflection}\\or a \textbf{non-stationary point of inflection}
	\end{itemize}
\end{frame}

\begin{frame}{Second derivative test}
	\textbf{What if $f''(x_0) = 0$?}
	\\~\\
	In other cases, we actually \textbf{can't make any conclusions}!
	\begin{itemize}
		\item Consider $y = x^4$ and $y = x^5$.
		\item \textbf{Both} of these satisfy $dy/dx = d^2y/dx^2 = 0$ at $x = 0$
		\begin{itemize}
			\item $y = x^4$ has a local minimum
			\item $y = x^5$ has an inflection.
			
		\end{itemize}
		\item \textbf{Thus}, in general we will need \textbf{further investigation} in these cases
	\end{itemize}	
\end{frame}

\begin{frame}{Second derivative test}
	\textbf{Problem}: find and classify all the stationary points of $y = x^2 e^{-x}$
	\vspace{5cm}
\end{frame}

\begin{frame}{Integration techniques}
	We will follow on from what we learned in Methods.\\~\\
	\textbf{Definition of antiderivative}\\~\\
	For a (sufficiently nice) function $f(x)$, we say $F(x)$ is an antiderivative of $f$ if
	$$\dfrac{dF}{dx} = f$$
\end{frame}

\begin{frame}{Integration techniques}
	\textbf{Connection between definite integrals and the antiderivative}\\~\\
	$$\int_a^b f(x) dx = F(b) - F(a)$$
	Where $F$ is \textbf{any} antiderivative of $f$.\\~\\
	Our definite integral itself can be interpreted as the (signed) area bounded by $f$ and the $x$-axis on the interval $[a, b]$
\end{frame}

\begin{frame}{Substitution of variables}
	This is a very powerful technique that allows us to compute \textbf{indefinite integrals} ($\equiv$ antiderivatives) of a wide range of functions.
	\\~\\
	Consider an integral of the general form:
	$$\int u'(t) f(u(t)) dt$$
	
	We can make a substitution $v = u(t) \Rightarrow dv = u'(t) dt$
	This brings us to
	
	$$\int u'(t) f(u(t)) dt = \int u'(t) f(u(t)) \dfrac{dv}{u'(t)} = \int f(v) dv$$
\end{frame}

\begin{frame}{Substitution of variables}
	\textbf{Problem: } Compute $$\int x e^{x^2} dx$$
	\vspace{5cm}
\end{frame}

\begin{frame}{Substitution of variables}
	\textbf{Problem: } Compute $$\int \dfrac{3 \d x}{x \ln^2(x)}$$
	\vspace{5cm}
\end{frame}

\begin{frame}{Substitution of variables}
	We can also perform substitution of variables for \textbf{definite integrals} - we just need to also substitute the terminals.
\end{frame}

\begin{frame}{Substitution of variables}
	\textbf{Problem (VCAA 2017 Exam 1):} Find $$\int_1^{\sqrt{3}} \dfrac{dx}{x(1+x^2)}$$
	\vspace{5cm}
\end{frame}

\begin{frame}{Integration of trig functions}
\textbf{Important identities}
\begin{itemize}
	\item $\cos^2(x) = \dfrac{1 + \cos(2x)}{2}$
	\item $\sin^2(x) = \dfrac{1 - \cos(2x)}{2}$
	\item $\sin(2x) = 2\sin(x)\cos(x)$
\end{itemize}

\textbf{Example: } $\displaystyle\int \sin^2(x)\cos^2(x) \d x$

\end{frame}

\begin{frame}{Other integration techniques}
    \textbf{Partial fractions..}\\~\\
    Typically used to integrate rational functions of the form $P(x)/Q(x)$ with $(\text{degree of P}) < (\text{degree of Q)}$\\~\\
    \textit{In the denominator...}
    \begin{itemize}
        \item Linear term $(ax+b)$ $\Rightarrow$ $\dfrac{A}{ax+b}$
        \item Repeated linear term $(ax+b)^n$ $\Rightarrow$ $\dfrac{A_1}{ax+b} + ... + \dfrac{A_n}{(ax+b)^n}$
        \item Quadratic linear term $ax^2+bx+c$ $\Rightarrow$ $\dfrac{Ax+B}{ax^2 + bx + c}$
    \end{itemize}
\end{frame}

\begin{frame}{Other integration techniques}
    \textbf{Problem: }\\~\\
    $$\int \dfrac{1}{(x^2 + 3x + 2)} \d x = \int \dfrac{1}{(x+2)(x+1)} \d x$$
    \vspace{5cm}
\end{frame}

\begin{frame}{Other integration techniques}
    \textbf{Problem: }\\~\\
    Find $\dfrac{d}{dx} x\ln x$. Using the result, compute $\displaystyle\int \ln x \d x$
    \vspace{5cm}
\end{frame}

\begin{frame}{Interesting problem}
    Compute $\dfrac{d}{dx} (\sin(x))^x$
    \vspace{5cm}
\end{frame}

\begin{frame}{Applications of integration}
\begin{itemize}
	\item A \textbf{very} cliche application of integration is to find areas
	\item $\displaystyle \int f(x) \d x$ can be interpreted as the (signed) area bound by $f(x)$ and the $x$-axis.
	\item This is understood generally according the \textbf{Riemann} sense of the integral\\
	$$ \int_a^b f(x) \d x := \lim_{n\to\infty} \left[\sum_{k = 1}^n f(x_k) \Delta_k\right]$$
\end{itemize}
\end{frame}

\begin{frame}
\includegraphics[width=\linewidth]{img/riemann.png}
\end{frame}

\begin{frame}{Volumes}
	Consider rotating a graph $f(x)$ about the $x$-axis by $2\pi$ rads.
	\begin{center}
	\includegraphics[width = 0.65\linewidth]{img/rotation.jpg}
	\end{center}
	
	Each 'disk' has a volume
	$$ \Delta V = \pi r^2 h = \pi(f(x))^2\Delta x$$
\end{frame}

\begin{frame}{Volumes}
	And so\\~\\
	\begin{align*}
		\text{volume of revolution} &= \lim_{n \to \infty} \sum_{1 \le k \le n} \pi (f(x))^2 \Delta x_k	\\
		&= \pi \int_a^b (f(x))^2 \d x
	\end{align*}
	\\~\\
	Thus, we have the general formula for the volume generated by rotating $y = f(x)$ by $2\pi$ rad around the $x$-axis:
	
	$$V = \pi \int_a^b y^2 dx$$
\end{frame}

\begin{frame}{Volumes}
	\textbf{Problem: } calculate the volume of the solid generated by rotating $y = x^2, 1\le x \le 2$, about the $x$-axis by $2\pi$ rad.
	\vspace{5cm}
\end{frame}	

\begin{frame}{Arc length}
	\textbf{Ever wondered how long a curve is?}\\~\\
	Here's our chance.
	
	Define a little piece of $x$ $\Rightarrow$ $\d x$\\
	... and a little piece of corresponding $y$-ness $\Rightarrow$ $\d y = f'(x) \d x$
	\\~\\
	Thus, by Pythagoras' theorem we have
	
	$$d\ell = \sqrt{dx^2 + dy^2}$$
\end{frame}

\begin{frame}{Arc length}
	And thus...
	
	\begin{align*}
		d\ell &= \sqrt{dx^2 + dy^2}\\
		\Rightarrow \dfrac{d\ell}{dx} &= \dfrac{1}{dx}\sqrt{dx^2 + dy^2}\\
		\Rightarrow \dfrac{d\ell}{dx} &= \sqrt{1 + \left(\dfrac{dy}{dx}\right)^2}\\
		\Rightarrow L &= \int_{x = a}^{x = b} d\ell\\
		&= \int_a^b \sqrt{1 + \left(\dfrac{dy}{dx}\right)^2} dx
	\end{align*}
\end{frame}

\begin{frame}{Arc length}
	Thus the \textbf{arc length} of a curve $y = f(x)$ between $x = a$ and $x = b$ is given by the formula
	
	$$ L = \int_a^b \sqrt{1 + \left(\dfrac{dy}{dx}\right)^2} dx $$
\end{frame}

\end{document}